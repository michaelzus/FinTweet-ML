"""Text cleaning and normalization for FinBERT processing.

This module provides text cleaning optimized for FinBERT sentiment analysis:
- Unicode normalization (NFKC) for fancy fonts
- Emoji-to-text mapping for sentiment preservation
- Punctuation normalization
- Removal of zero-width and invisible characters
"""

import re
import unicodedata
from typing import Dict

# =============================================================================
# Finance-specific emoji mappings
# Based on actual data analysis of 34,899 tweets with 49,044 emoji occurrences
# =============================================================================

FINANCE_EMOJI_MAP: Dict[str, str] = {
    # Market direction indicators (high frequency)
    "ðŸŸ¢": "[green]",  # 11,879x - bullish/up signal
    "ðŸ”´": "[red]",  # 3,919x - bearish/down signal
    "ðŸŸ¡": "[yellow]",  # 647x - neutral/caution
    "ðŸ“ˆ": "[up]",  # 209x - uptrend
    "ðŸ“‰": "[down]",  # 126x - downtrend
    "ðŸ”¼": "[up]",  # 6x - up triangle
    "ðŸ”½": "[down]",  # 6x - down triangle
    "ðŸ”»": "[down]",  # 17x - down red triangle
    "ðŸŸ ": "[orange]",  # 7x - caution
    "ðŸ”µ": "[blue]",  # 7x - neutral
    # Sentiment signals (valuable for FinBERT)
    "ðŸš€": "[rocket]",  # 251x - bullish momentum
    "ðŸ”¥": "[fire]",  # 291x - hot/trending
    "ðŸ’©": "[bad]",  # 97x - negative sentiment
    "ðŸ˜­": "[crying]",  # 73x - panic/despair
    "ðŸ˜‚": "[laughing]",  # 198x - mockery/disbelief
    "ðŸ¤£": "[laughing]",  # 49x - same
    "ðŸ˜": "[neutral]",  # 59x - flat/unchanged
    "ðŸ¥¶": "[cold]",  # 115x - frozen/dead stock
    "ðŸ’€": "[dead]",  # 18x - death cross/terrible
    "âœ…": "[check]",  # 178x - confirmed/positive
    "âŒ": "[x]",  # 54x - negative/denied
    "ðŸ‘€": "[eyes]",  # 138x - watching/attention
    "ðŸ¤”": "[thinking]",  # 28x - uncertain
    "ðŸ™Œ": "[celebrate]",  # 49x - bullish celebration
    "ðŸ™": "[hope]",  # 45x - hoping/praying
    "ðŸ‘‘": "[king]",  # 60x - best performer
    "ðŸ†": "[trophy]",  # 9x - winner
    "ðŸŽ¯": "[target]",  # 17x - price target hit
    "ðŸ¤¯": "[shocked]",  # 33x - surprised
    "ðŸ˜³": "[surprised]",  # 44x - flushed/shocked
    "ðŸ˜‘": "[flat]",  # 26x - expressionless
    "ðŸ˜•": "[confused]",  # 25x - uncertain
    "ðŸ˜®": "[wow]",  # 9x - surprised
    "ðŸ˜²": "[shocked]",  # 4x - astonished
    "ðŸ˜…": "[nervous]",  # 4x - nervous laugh
    "ðŸ˜‰": "[wink]",  # 16x - confident
    "ðŸ˜Ž": "[cool]",  # 5x - confident
    "ðŸ«¡": "[salute]",  # 12x - respect
    "ðŸ¤": "[handshake]",  # 19x - deal/agreement
    "ðŸ’ª": "[strong]",  # 1x - strength
    "â¤": "[heart]",  # 16x - love
    "ðŸ’š": "[green_heart]",  # 6x - bullish love
    "ðŸ¥²": "[bittersweet]",  # 5x - mixed feelings
    "ðŸ¥¹": "[emotional]",  # 4x - holding back tears
    "ðŸ«£": "[peeking]",  # 6x - cautious look
    "ðŸ¤·": "[shrug]",  # 4x - uncertain
    # Finance-specific context
    "ðŸ“Š": "[chart]",  # 562x - data/analysis
    "ðŸ¦": "[bank]",  # 44x - banking sector
    "ðŸ’°": "[money]",  # 55x - money/profit
    "ðŸ’¸": "[money_fly]",  # 17x - money leaving
    "ðŸ’³": "[card]",  # 55x - credit/payment
    "ðŸ’¼": "[business]",  # 13x - corporate
    "ðŸª™": "[coin]",  # 12x - crypto
    "ðŸ’µ": "[dollar]",  # 2x - cash
    "ðŸ’¹": "[chart_yen]",  # 5x - markets
    "ðŸ’¯": "[hundred]",  # 2x - perfect
    "â™¾": "[infinity]",  # 12x - unlimited
    # Sector indicators
    "ðŸ’Š": "[pharma]",  # 44x - healthcare/pharma
    "ðŸ’‰": "[vaccine]",  # 7x - healthcare
    "ðŸ¦ ": "[virus]",  # 6x - biotech
    "ðŸ§«": "[biotech]",  # 2x - lab
    "ðŸ¥": "[hospital]",  # 6x - healthcare
    "âš•": "[medical]",  # 3x - healthcare
    "ðŸ¤–": "[ai]",  # 33x - AI/tech
    "ðŸ“±": "[mobile]",  # 83x - tech/mobile
    "ðŸ’»": "[computer]",  # 51x - tech
    "ðŸ–¥": "[desktop]",  # 36x - tech
    "â˜": "[cloud]",  # 86x - cloud computing
    "âš¡": "[energy]",  # 35x - energy/power
    "ðŸ›¢": "[oil]",  # 8x - oil/energy
    "ðŸš—": "[auto]",  # 13x - automotive
    "ðŸš–": "[taxi]",  # 14x - ride-sharing
    "ðŸš˜": "[car]",  # 1x - automotive
    "ðŸ›©": "[airline]",  # 19x - aviation
    "âœˆ": "[plane]",  # airline
    "ðŸš¢": "[ship]",  # 1x - shipping
    "ðŸ›³": "[cruise]",  # 6x - cruise lines
    "ðŸšš": "[truck]",  # 8x - logistics
    "ðŸˆ": "[football]",  # 17x - sports betting
    "ðŸŽ®": "[gaming]",  # 23x - gaming sector
    "ðŸ•¹": "[joystick]",  # 10x - gaming
    "ðŸ": "[apple]",  # 15x - AAPL
    "ðŸŽ": "[apple_red]",  # 15x - AAPL
    "ðŸ¶": "[doge]",  # 20x - DOGE/meme stocks
    "ðŸ¦…": "[eagle]",  # 5x - America/patriotic
    "ðŸŒŠ": "[wave]",  # 5x - momentum
    "ðŸ§ ": "[brain]",  # 15x - AI/smart
    "ðŸ¦¾": "[robot_arm]",  # 14x - automation
    "ðŸ¦¿": "[robot_leg]",  # 1x - automation
    "ðŸ¢": "[office]",  # 14x - real estate
    "ðŸ°": "[castle]",  # 13x - fortress
    "ðŸ­": "[factory]",  # 4x - manufacturing
    "ðŸ—": "[construction]",  # 9x - building
    "ðŸ‘·": "[worker]",  # 6x - labor
    "ðŸ›’": "[shopping]",  # 23x - retail/e-commerce
    "ðŸ›": "[bags]",  # 12x - retail
    "ðŸ“¦": "[package]",  # 31x - delivery/logistics
    "ðŸ“º": "[tv]",  # 35x - media
    "ðŸŽ¬": "[movie]",  # 8x - entertainment
    "ðŸŽ¥": "[camera]",  # 3x - media
    "ðŸŽ§": "[headphones]",  # 11x - audio/streaming
    "ðŸŽ¤": "[mic]",  # 4x - media
    "ðŸŽ™": "[podcast]",  # 1x - media
    "ðŸŒ": "[globe]",  # 49x - global/international
    "ðŸŒ": "[earth]",  # 3x - global
    "ðŸŒŽ": "[americas]",  # 3x - US markets
    "ðŸŒ": "[asia]",  # 6x - Asian markets
    "ðŸ•": "[pizza]",  # 15x - food sector
    "ðŸŸ": "[fries]",  # 8x - fast food
    "ðŸ”": "[burger]",  # 4x - fast food
    "ðŸŒ®": "[taco]",  # 3x - fast food
    "ðŸŒ¯": "[burrito]",  # 6x - food
    "ðŸ¥¤": "[drink]",  # 33x - beverages
    "â˜•": "[coffee]",  # 8x - coffee/Starbucks
    "ðŸº": "[beer]",  # 10x - alcohol
    "ðŸ¿": "[popcorn]",  # 20x - entertainment
    "ðŸª": "[cookie]",  # 4x - food
    "ðŸ¥•": "[carrot]",  # 7x - food/health
    "ðŸŒ¿": "[herb]",  # 8x - cannabis?
    "ðŸ„": "[mushroom]",  # 2x - psychedelics?
    "ðŸŒ±": "[seedling]",  # 6x - growth/ESG
    "ðŸŒŸ": "[star]",  # 27x - highlight
    "â­": "[star]",  # star
    "ðŸ”¬": "[research]",  # 12x - R&D
    "ðŸ”": "[secure]",  # 13x - security
    "ðŸ”’": "[lock]",  # 12x - security
    "ðŸ”‘": "[key]",  # key
    "ðŸ“¶": "[signal]",  # 10x - telecom
    "ðŸ“¡": "[antenna]",  # 1x - telecom
    "ðŸ›°": "[satellite]",  # 4x - space
    "ðŸ’¡": "[idea]",  # 11x - innovation
    "âš™": "[gear]",  # 44x - settings/engineering
    "ðŸ› ": "[tools]",  # 9x - maintenance
    "âš–": "[scales]",  # 2x - legal/balance
    "ðŸ§´": "[lotion]",  # 12x - consumer goods
    "ðŸ‘Ÿ": "[shoe]",  # 18x - retail/Nike
    "ðŸ‘”": "[tie]",  # 14x - business
    "ðŸ‘œ": "[handbag]",  # 6x - luxury
    "ðŸ‘“": "[glasses]",  # 3x - eyewear
    "ðŸŽ¿": "[ski]",  # 2x - leisure
    "â›·": "[skier]",  # 4x - leisure
    "ðŸ§˜": "[yoga]",  # 18x - wellness
    "ðŸ›Œ": "[sleep]",  # 2x - rest
    "ðŸ”®": "[crystal]",  # 2x - prediction
    "ðŸ": "[goat]",  # 2x - greatest of all time
    "ðŸ¦ˆ": "[shark]",  # 1x - predator
    "ðŸ³": "[whale]",  # 1x - big investor
    "ðŸ¦š": "[peacock]",  # 3x - NBC?
    "ðŸ¦Ž": "[lizard]",  # 4x - gecko?
    "ðŸ": "[island]",  # 4x - vacation
    "â›°": "[mountain]",  # 3x - obstacle
    "ðŸŒª": "[tornado]",  # 1x - chaos
    "ðŸŒ€": "[cyclone]",  # 1x - chaos
    "â„": "[cold]",  # 18x - frozen
    "ðŸ§¨": "[explosive]",  # 1x - volatile
    "ðŸ’¥": "[explosion]",  # 1x - breakout
    "ðŸ’¨": "[dash]",  # 3x - fast
    "ðŸ”Š": "[loud]",  # 2x - announcement
    "ðŸ”‹": "[battery]",  # 1x - energy storage
    "ðŸ›ž": "[wheel]",  # 1x - automotive
    "ðŸ«’": "[olive]",  # 1x - food
    # Rankings/medals
    "ðŸ¥‡": "[first]",  # 337x - top performer
    "ðŸ¥ˆ": "[second]",  # 386x - second place
    "ðŸ¥‰": "[third]",  # 386x - third place
    # Formatting emojis - remove (no semantic value for FinBERT)
    "ðŸ”¹": "",  # 10,789x - bullet decoration
    "ðŸ”¸": "",  # 2,475x - bullet decoration
    "ðŸ”·": "",  # 20x - diamond decoration
    "ðŸ“¢": "",  # 5,459x - announcement marker
    "ðŸš¨": "[alert]",  # 2,733x - breaking news (keep as alert)
    "âž¤": "",  # 2,197x - arrow decoration
    "âž¡": "",  # 765x - arrow
    "ðŸ‘‡": "",  # 1,874x - "see below"
    "ðŸ‘‰": "",  # 433x - "see this"
    "ðŸ‘ˆ": "",  # 2x - "see left"
    "ðŸ§µ": "[thread]",  # 48x - thread marker
    "ðŸ””": "",  # 72x - notification bell
    "ðŸ—“": "",  # 65x - calendar
    "ðŸ“†": "",  # 12x - calendar
    "ðŸ“": "",  # 7x - memo
    "âœ": "",  # 10x - writing
    "ðŸ”": "",  # 9x - search
    "ðŸ”Ž": "",  # 1x - search
    "ðŸ”—": "",  # 1x - link
    "ðŸ“Œ": "",  # 1x - pin
    "ðŸŽŸ": "",  # 1x - ticket
    "âœ”": "[check]",  # 7x - checkmark
    "ðŸ”˜": "",  # 1x - radio button
    "ðŸ”ƒ": "",  # 4x - refresh
    "ðŸ”„": "",  # 1x - refresh
    "ðŸ†•": "",  # 6x - new
    "ðŸŸ¥": "",  # 1x - red square
    "â¬›": "",  # 1x - black square
    "â–ª": "",  # 97x - small square
    "â–«": "",  # 19x - white square
    "â—¾": "",  # 6x - medium square
    "â—": "",  # 1x - circle
    "âš«": "",  # 4x - black circle
    "ðŸ•µ": "",  # 15x - spy
    "ðŸ‘¨": "",  # 3x - man
    "ðŸ‘©": "",  # 3x - woman
    "ðŸ‘§": "",  # 3x - girl
    "ðŸ‘¦": "",  # 3x - boy
    "ðŸ§‘": "",  # 14x - adult
    "ðŸ‘": "",  # 1x - open hands
    "ðŸ¤³": "",  # 1x - selfie
    "ðŸ¤¦": "",  # 1x - facepalm
    "ðŸ¤ž": "",  # 1x - crossed fingers
    "ðŸ§": "",  # 1x - monocle
    "ðŸ¥½": "",  # 1x - goggles
    "ðŸ˜": "",  # 1x - heart eyes
    "ðŸ˜‹": "",  # 1x - yummy
    "ðŸ˜ª": "",  # 2x - sleepy
    "ðŸ˜": "",  # 2x - smirk
    "ðŸ¥º": "",  # 1x - pleading
    "ðŸŽ": "",  # 3x - race car
    "ðŸ’Ž": "[diamond]",  # 1x - diamond hands
    "â™Ÿ": "",  # 1x - chess pawn
    "ðŸ—¡": "",  # 1x - dagger
    # Variation selector - always remove (invisible modifier)
    "ï¸": "",  # 1,489x - variation selector-16
    # Skin tone modifiers - remove
    "ðŸ»": "",  # 18x - light skin
    "ðŸ¼": "",  # 1x - medium-light skin
    # Male/female signs
    "â™‚": "",  # 5x - male sign
}

# Regional indicator letters (flags decomposed) - all map to empty string
REGIONAL_INDICATORS: Dict[str, str] = {chr(c): "" for c in range(0x1F1E6, 0x1F200)}

# =============================================================================
# Punctuation normalization
# Smart quotes, dashes, bullets -> ASCII equivalents
# =============================================================================

PUNCTUATION_MAP: Dict[str, str] = {
    "'": "'",  # U+2019 RIGHT SINGLE QUOTATION MARK (13,128x)
    "'": "'",  # U+2018 LEFT SINGLE QUOTATION MARK (322x)
    """: '"',    # U+201C LEFT DOUBLE QUOTATION MARK (3,305x)
    """: '"',  # U+201D RIGHT DOUBLE QUOTATION MARK (3,292x)
    "â€”": " - ",  # U+2014 EM DASH (4,451x) - with spaces for readability
    "â€“": "-",  # U+2013 EN DASH (4,205x)
    "â€¦": "...",  # U+2026 HORIZONTAL ELLIPSIS (1,488x)
    "â€¢": " - ",  # U+2022 BULLET (2,425x)
    "â€£": " - ",  # U+2023 TRIANGULAR BULLET (25x)
    "â€‘": "-",  # U+2011 NON-BREAKING HYPHEN (23x)
    "âˆ™": "-",  # U+2219 BULLET OPERATOR (2x)
    "ï¼š": ":",  # U+FF1A FULLWIDTH COLON (4x)
}

# =============================================================================
# Arrow symbols -> directional tokens or removal
# =============================================================================

ARROW_MAP: Dict[str, str] = {
    "â†‘": "[up]",  # U+2191 UPWARDS ARROW (232x)
    "â†“": "[down]",  # U+2193 DOWNWARDS ARROW (70x)
    "â¬†": "[up]",  # U+2B06 UPWARDS BLACK ARROW (28x)
    "â¬‡": "[down]",  # U+2B07 DOWNWARDS BLACK ARROW (138x)
    "â–²": "[up]",  # U+25B2 BLACK UP-POINTING TRIANGLE (72x)
    "â–¼": "[down]",  # U+25BC BLACK DOWN-POINTING TRIANGLE (24x)
    "â†—": "[up]",  # U+2197 NORTH EAST ARROW (9x)
    "â†’": "",  # U+2192 RIGHTWARDS ARROW (134x) - formatting
    "â–¶": "",  # U+25B6 BLACK RIGHT-POINTING TRIANGLE (6x)
    "â†”": "",  # U+2194 LEFT RIGHT ARROW (2x)
    "â†³": "",  # U+21B3 DOWNWARDS ARROW WITH TIP RIGHTWARDS (1x)
    "â¬…": "",  # U+2B05 LEFTWARDS BLACK ARROW (1x)
}

# =============================================================================
# Zero-width and invisible characters to remove
# =============================================================================

ZERO_WIDTH_CHARS: set[str] = {
    "\u200b",  # ZERO WIDTH SPACE (32x)
    "\u200c",  # ZERO WIDTH NON-JOINER (5x)
    "\u200d",  # ZERO WIDTH JOINER (31x)
    "\u2060",  # WORD JOINER (1x)
    "\ufffc",  # OBJECT REPLACEMENT CHARACTER (1x)
    "\u20e3",  # COMBINING ENCLOSING KEYCAP (3x)
}

# Whitespace variants to normalize to regular space
WHITESPACE_VARIANTS: Dict[str, str] = {
    "\u202f": " ",  # NARROW NO-BREAK SPACE (441x)
    "\u00a0": " ",  # NO-BREAK SPACE (54x)
    "\u2009": " ",  # THIN SPACE (4x)
    "\u2002": " ",  # EN SPACE (2x)
}

# =============================================================================
# Regex patterns
# =============================================================================

# Pattern for remaining emojis not in our mapping (to remove)
EMOJI_PATTERN = re.compile(
    "["
    "\U0001f300-\U0001f9ff"  # Misc Symbols, Emoticons, etc.
    "\U00002600-\U000027bf"  # Dingbats, Misc symbols
    "\U0001fa00-\U0001faff"  # Supplemental Symbols
    "\U0001f600-\U0001f64f"  # Emoticons
    "\U0001f1e0-\U0001f1ff"  # Regional indicators (flags)
    "\U0001f000-\U0001f02f"  # Mahjong, dominos
    "\U0000fe00-\U0000fe0f"  # Variation selectors
    "\U000e0000-\U000e007f"  # Tags
    "]+"
)

# Pattern to collapse multiple spaces
MULTI_SPACE_PATTERN = re.compile(r" {2,}")


def clean_for_finbert(text: str) -> str:
    """
    Clean and normalize text for FinBERT processing.

    Applies the following transformations in order:
    1. NFKC Unicode normalization (fixes fancy fonts)
    2. Remove zero-width characters
    3. Normalize whitespace variants
    4. Normalize punctuation (smart quotes, dashes)
    5. Map arrows to directional tokens
    6. Map finance emojis to semantic tokens
    7. Remove regional indicators (flag components)
    8. Remove remaining unmapped emojis
    9. Collapse multiple spaces

    Args:
        text: Raw text input

    Returns:
        Cleaned text optimized for FinBERT tokenization
    """
    if not text:
        return text

    # 1. NFKC normalization - converts mathematical bold, compatibility chars
    # This handles ~57k occurrences of fancy Twitter fonts
    text = unicodedata.normalize("NFKC", text)

    # 2. Remove zero-width characters
    for char in ZERO_WIDTH_CHARS:
        text = text.replace(char, "")

    # 3. Normalize whitespace variants to regular space
    for char, replacement in WHITESPACE_VARIANTS.items():
        text = text.replace(char, replacement)

    # 4. Normalize punctuation
    for char, replacement in PUNCTUATION_MAP.items():
        text = text.replace(char, replacement)

    # 5. Map arrows to directional tokens
    for char, replacement in ARROW_MAP.items():
        text = text.replace(char, replacement)

    # 6. Map finance emojis to semantic tokens
    for emoji, token in FINANCE_EMOJI_MAP.items():
        if emoji in text:
            # Add spaces around tokens to ensure proper tokenization
            replacement = f" {token} " if token else ""
            text = text.replace(emoji, replacement)

    # 7. Remove regional indicators (flag components)
    for char in REGIONAL_INDICATORS:
        text = text.replace(char, "")

    # 8. Remove any remaining unmapped emojis
    text = EMOJI_PATTERN.sub("", text)

    # 9. Collapse multiple spaces and strip
    text = MULTI_SPACE_PATTERN.sub(" ", text)
    text = text.strip()

    return text
