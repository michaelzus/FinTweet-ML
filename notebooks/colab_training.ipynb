{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FinBERT Tweet Classifier Training - Google Colab\n",
        "\n",
        "This notebook allows you to train the FinBERT-based multi-modal tweet classifier on Google Colab with GPU acceleration.\n",
        "\n",
        "**Before running:**\n",
        "1. Go to `Runtime` → `Change runtime type` → Select `GPU` (T4 recommended)\n",
        "2. Upload your training data CSV file when prompted\n",
        "\n",
        "**Requirements:**\n",
        "- Enriched CSV data file (e.g., `15-dec-enrich7.csv`)\n",
        "- Google Colab with GPU enabled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q transformers>=4.30.0 accelerate>=0.26.0 datasets>=2.14.0 scikit-learn>=1.3.0 torch>=2.0.0 seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import logging\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertModel, BertTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION - Modify these settings as needed\n",
        "# =============================================================================\n",
        "\n",
        "# Target Configuration\n",
        "TARGET_COLUMN = \"label_1d_3class\"  # 1-day labels\n",
        "LABEL_MAP = {\"SELL\": 0, \"HOLD\": 1, \"BUY\": 2}\n",
        "LABEL_MAP_INV = {0: \"SELL\", 1: \"HOLD\", 2: \"BUY\"}\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "# Feature Configuration\n",
        "NUMERICAL_FEATURES = [\n",
        "    # Core indicators (baseline)\n",
        "    \"volatility_7d\",\n",
        "    \"relative_volume\",\n",
        "    \"rsi_14\",\n",
        "    \"distance_from_ma_20\",\n",
        "    # Phase 2: Multi-period momentum\n",
        "    \"return_5d\",\n",
        "    \"return_20d\",\n",
        "    # Phase 2: Trend confirmation\n",
        "    \"above_ma_20\",\n",
        "    \"slope_ma_20\",\n",
        "    # Phase 2: Shock/Gap features\n",
        "    \"gap_open\",\n",
        "    \"intraday_range\",\n",
        "]\n",
        "\n",
        "TEXT_COLUMN = \"text\"\n",
        "\n",
        "# Model Configuration\n",
        "FINBERT_MODEL_NAME = \"yiyanghkust/finbert-tone\"\n",
        "MAX_TEXT_LENGTH = 128\n",
        "\n",
        "# Embedding dimensions\n",
        "AUTHOR_EMBEDDING_DIM = 16\n",
        "CATEGORY_EMBEDDING_DIM = 8\n",
        "MARKET_REGIME_EMBEDDING_DIM = 4\n",
        "SECTOR_EMBEDDING_DIM = 8\n",
        "MARKET_CAP_EMBEDDING_DIM = 4\n",
        "NUMERICAL_HIDDEN_DIM = 32\n",
        "\n",
        "# Training Defaults - ADJUST THESE FOR COLAB\n",
        "DEFAULT_BATCH_SIZE = 16  # Increase to 32 if you have a T4/V100\n",
        "DEFAULT_LEARNING_RATE = 2e-5\n",
        "DEFAULT_NUM_EPOCHS = 5\n",
        "DEFAULT_DROPOUT = 0.3\n",
        "DEFAULT_WARMUP_RATIO = 0.1\n",
        "DEFAULT_WEIGHT_DECAY = 0.01\n",
        "\n",
        "# Data Split Configuration\n",
        "DEFAULT_TEST_SIZE = 0.15\n",
        "DEFAULT_VAL_SIZE = 0.15\n",
        "RANDOM_SEED = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Upload Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Upload from local machine\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload your enriched CSV file:\")\n",
        "uploaded = files.upload()\n",
        "DATA_FILE = list(uploaded.keys())[0]\n",
        "print(f\"\\nUploaded: {DATA_FILE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 2: Mount Google Drive (uncomment if you prefer this method)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# DATA_FILE = '/content/drive/MyDrive/path/to/your/15-dec-enrich7.csv'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Loading & Processing Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_enriched_data(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load enriched tweet data from CSV.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    logger.info(f\"Loaded {len(df)} rows from {file_path}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def filter_reliable(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Filter to reliable samples with valid targets.\"\"\"\n",
        "    # Must have target label\n",
        "    df = df[df[TARGET_COLUMN].notna()].copy()\n",
        "    \n",
        "    # Must have text\n",
        "    df = df[df[TEXT_COLUMN].notna()].copy()\n",
        "    df = df[df[TEXT_COLUMN].str.len() > 0].copy()\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def split_by_hash(\n",
        "    df: pd.DataFrame,\n",
        "    test_size: float = DEFAULT_TEST_SIZE,\n",
        "    val_size: float = DEFAULT_VAL_SIZE,\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Split data by tweet hash for reproducibility.\"\"\"\n",
        "    if \"tweet_hash\" not in df.columns:\n",
        "        # Create hash from text if not present\n",
        "        df = df.copy()\n",
        "        df[\"tweet_hash\"] = df[TEXT_COLUMN].apply(\n",
        "            lambda x: hashlib.md5(str(x).encode()).hexdigest()\n",
        "        )\n",
        "    \n",
        "    # Use first 8 chars of hash as integer for deterministic split\n",
        "    df = df.copy()\n",
        "    df[\"hash_int\"] = df[\"tweet_hash\"].apply(lambda x: int(x[:8], 16) % 1000)\n",
        "    \n",
        "    # Split boundaries\n",
        "    test_thresh = int(test_size * 1000)\n",
        "    val_thresh = int((test_size + val_size) * 1000)\n",
        "    \n",
        "    df_test = df[df[\"hash_int\"] < test_thresh].drop(columns=[\"hash_int\"])\n",
        "    df_val = df[(df[\"hash_int\"] >= test_thresh) & (df[\"hash_int\"] < val_thresh)].drop(columns=[\"hash_int\"])\n",
        "    df_train = df[df[\"hash_int\"] >= val_thresh].drop(columns=[\"hash_int\"])\n",
        "    \n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "\n",
        "def compute_class_weights(labels: pd.Series) -> Dict[int, float]:\n",
        "    \"\"\"Compute inverse frequency class weights.\"\"\"\n",
        "    if labels.dtype == object:\n",
        "        labels = labels.map(LABEL_MAP)\n",
        "    \n",
        "    counts = labels.value_counts()\n",
        "    total = len(labels)\n",
        "    n_classes = len(counts)\n",
        "    \n",
        "    weights = {}\n",
        "    for cls in range(n_classes):\n",
        "        if cls in counts.index:\n",
        "            weights[cls] = total / (n_classes * counts[cls])\n",
        "        else:\n",
        "            weights[cls] = 1.0\n",
        "    \n",
        "    return weights\n",
        "\n",
        "\n",
        "def weights_to_tensor(weights: Dict[int, float]) -> torch.Tensor:\n",
        "    \"\"\"Convert weight dict to tensor.\"\"\"\n",
        "    return torch.tensor([weights[i] for i in range(len(weights))], dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TweetDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for tweet classification with multi-modal features.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: Union[pd.Series, List[str]],\n",
        "        numerical_features: Union[pd.DataFrame, np.ndarray],\n",
        "        author_indices: Union[pd.Series, np.ndarray],\n",
        "        category_indices: Union[pd.Series, np.ndarray],\n",
        "        market_regime_indices: Union[pd.Series, np.ndarray],\n",
        "        sector_indices: Union[pd.Series, np.ndarray],\n",
        "        market_cap_indices: Union[pd.Series, np.ndarray],\n",
        "        labels: Union[pd.Series, np.ndarray],\n",
        "        tokenizer,\n",
        "        max_length: int = MAX_TEXT_LENGTH,\n",
        "    ):\n",
        "        if isinstance(texts, pd.Series):\n",
        "            texts = texts.tolist()\n",
        "\n",
        "        self.encodings = tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        if isinstance(numerical_features, pd.DataFrame):\n",
        "            numerical_features = numerical_features.values\n",
        "        self.numerical = torch.tensor(numerical_features, dtype=torch.float32)\n",
        "\n",
        "        if isinstance(author_indices, pd.Series):\n",
        "            author_indices = author_indices.values\n",
        "        if isinstance(category_indices, pd.Series):\n",
        "            category_indices = category_indices.values\n",
        "        if isinstance(market_regime_indices, pd.Series):\n",
        "            market_regime_indices = market_regime_indices.values\n",
        "        if isinstance(sector_indices, pd.Series):\n",
        "            sector_indices = sector_indices.values\n",
        "        if isinstance(market_cap_indices, pd.Series):\n",
        "            market_cap_indices = market_cap_indices.values\n",
        "\n",
        "        self.author_idx = torch.tensor(author_indices, dtype=torch.long)\n",
        "        self.category_idx = torch.tensor(category_indices, dtype=torch.long)\n",
        "        self.market_regime_idx = torch.tensor(market_regime_indices, dtype=torch.long)\n",
        "        self.sector_idx = torch.tensor(sector_indices, dtype=torch.long)\n",
        "        self.market_cap_idx = torch.tensor(market_cap_indices, dtype=torch.long)\n",
        "\n",
        "        if isinstance(labels, pd.Series):\n",
        "            labels = labels.values\n",
        "        if isinstance(labels[0], str):\n",
        "            labels = np.array([LABEL_MAP[label] for label in labels])\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        return {\n",
        "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
        "            \"numerical\": self.numerical[idx],\n",
        "            \"author_idx\": self.author_idx[idx],\n",
        "            \"category_idx\": self.category_idx[idx],\n",
        "            \"market_regime_idx\": self.market_regime_idx[idx],\n",
        "            \"sector_idx\": self.sector_idx[idx],\n",
        "            \"market_cap_idx\": self.market_cap_idx[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_categorical_encodings(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Create mappings from categorical values to indices.\"\"\"\n",
        "    authors = df[\"author\"].unique().tolist()\n",
        "    categories = df[\"category\"].unique().tolist()\n",
        "    market_regimes = df[\"market_regime\"].fillna(\"calm\").unique().tolist()\n",
        "    sectors = df[\"sector\"].fillna(\"Other\").unique().tolist()\n",
        "    market_caps = df[\"market_cap_bucket\"].fillna(\"unknown\").unique().tolist()\n",
        "\n",
        "    return {\n",
        "        \"author_to_idx\": {auth: i for i, auth in enumerate(authors)},\n",
        "        \"category_to_idx\": {cat: i for i, cat in enumerate(categories)},\n",
        "        \"market_regime_to_idx\": {reg: i for i, reg in enumerate(market_regimes)},\n",
        "        \"sector_to_idx\": {sec: i for i, sec in enumerate(sectors)},\n",
        "        \"market_cap_to_idx\": {cap: i for i, cap in enumerate(market_caps)},\n",
        "        \"num_authors\": len(authors),\n",
        "        \"num_categories\": len(categories),\n",
        "        \"num_market_regimes\": len(market_regimes),\n",
        "        \"num_sectors\": len(sectors),\n",
        "        \"num_market_caps\": len(market_caps),\n",
        "    }\n",
        "\n",
        "\n",
        "def encode_categorical(\n",
        "    df: pd.DataFrame,\n",
        "    author_to_idx: Dict[str, int],\n",
        "    category_to_idx: Dict[str, int],\n",
        "    market_regime_to_idx: Dict[str, int],\n",
        "    sector_to_idx: Dict[str, int],\n",
        "    market_cap_to_idx: Dict[str, int],\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Encode categorical columns to indices.\"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"market_regime\"] = df[\"market_regime\"].fillna(\"calm\")\n",
        "    df[\"sector\"] = df[\"sector\"].fillna(\"Other\")\n",
        "    df[\"market_cap_bucket\"] = df[\"market_cap_bucket\"].fillna(\"unknown\")\n",
        "\n",
        "    df[\"author_idx\"] = df[\"author\"].map(lambda x: author_to_idx.get(x, 0))\n",
        "    df[\"category_idx\"] = df[\"category\"].map(lambda x: category_to_idx.get(x, 0))\n",
        "    df[\"market_regime_idx\"] = df[\"market_regime\"].map(lambda x: market_regime_to_idx.get(x, 0))\n",
        "    df[\"sector_idx\"] = df[\"sector\"].map(lambda x: sector_to_idx.get(x, 0))\n",
        "    df[\"market_cap_idx\"] = df[\"market_cap_bucket\"].map(lambda x: market_cap_to_idx.get(x, 0))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_dataset_from_df(\n",
        "    df: pd.DataFrame,\n",
        "    tokenizer,\n",
        "    encodings: Dict[str, Any],\n",
        "    scaler: Optional[StandardScaler] = None,\n",
        "    fit_scaler: bool = False,\n",
        ") -> Tuple[TweetDataset, StandardScaler]:\n",
        "    \"\"\"Create TweetDataset from DataFrame.\"\"\"\n",
        "    df = encode_categorical(\n",
        "        df,\n",
        "        encodings[\"author_to_idx\"],\n",
        "        encodings[\"category_to_idx\"],\n",
        "        encodings[\"market_regime_to_idx\"],\n",
        "        encodings[\"sector_to_idx\"],\n",
        "        encodings[\"market_cap_to_idx\"],\n",
        "    )\n",
        "\n",
        "    numerical = df[NUMERICAL_FEATURES].fillna(0).values\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        fit_scaler = True\n",
        "\n",
        "    if fit_scaler:\n",
        "        numerical = scaler.fit_transform(numerical)\n",
        "    else:\n",
        "        numerical = scaler.transform(numerical)\n",
        "\n",
        "    dataset = TweetDataset(\n",
        "        texts=df[TEXT_COLUMN],\n",
        "        numerical_features=numerical,\n",
        "        author_indices=df[\"author_idx\"],\n",
        "        category_indices=df[\"category_idx\"],\n",
        "        market_regime_indices=df[\"market_regime_idx\"],\n",
        "        sector_indices=df[\"sector_idx\"],\n",
        "        market_cap_indices=df[\"market_cap_idx\"],\n",
        "        labels=df[TARGET_COLUMN],\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    return dataset, scaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FinBERTMultiModal(nn.Module):\n",
        "    \"\"\"FinBERT with numerical + categorical feature fusion.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_numerical_features: int,\n",
        "        num_authors: int,\n",
        "        num_categories: int,\n",
        "        num_market_regimes: int = 5,\n",
        "        num_sectors: int = 12,\n",
        "        num_market_caps: int = 5,\n",
        "        num_classes: int = NUM_CLASSES,\n",
        "        finbert_model: str = FINBERT_MODEL_NAME,\n",
        "        freeze_bert: bool = False,\n",
        "        dropout: float = DEFAULT_DROPOUT,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store config\n",
        "        self.num_numerical_features = num_numerical_features\n",
        "        self.num_authors = num_authors\n",
        "        self.num_categories = num_categories\n",
        "        self.num_market_regimes = num_market_regimes\n",
        "        self.num_sectors = num_sectors\n",
        "        self.num_market_caps = num_market_caps\n",
        "        self.num_classes = num_classes\n",
        "        self.finbert_model_name = finbert_model\n",
        "        self.freeze_bert = freeze_bert\n",
        "        self.dropout_prob = dropout\n",
        "\n",
        "        # FinBERT encoder\n",
        "        self.bert = BertModel.from_pretrained(finbert_model)\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        bert_hidden_size = self.bert.config.hidden_size  # 768\n",
        "\n",
        "        # Categorical embeddings\n",
        "        self.author_embedding = nn.Embedding(num_authors, AUTHOR_EMBEDDING_DIM)\n",
        "        self.category_embedding = nn.Embedding(num_categories, CATEGORY_EMBEDDING_DIM)\n",
        "        self.market_regime_embedding = nn.Embedding(num_market_regimes, MARKET_REGIME_EMBEDDING_DIM)\n",
        "        self.sector_embedding = nn.Embedding(num_sectors, SECTOR_EMBEDDING_DIM)\n",
        "        self.market_cap_embedding = nn.Embedding(num_market_caps, MARKET_CAP_EMBEDDING_DIM)\n",
        "\n",
        "        # Numerical feature encoder\n",
        "        self.numerical_encoder = nn.Sequential(\n",
        "            nn.Linear(num_numerical_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, NUMERICAL_HIDDEN_DIM),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Fusion + classifier\n",
        "        fusion_size = (\n",
        "            bert_hidden_size\n",
        "            + NUMERICAL_HIDDEN_DIM\n",
        "            + AUTHOR_EMBEDDING_DIM\n",
        "            + CATEGORY_EMBEDDING_DIM\n",
        "            + MARKET_REGIME_EMBEDDING_DIM\n",
        "            + SECTOR_EMBEDDING_DIM\n",
        "            + MARKET_CAP_EMBEDDING_DIM\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fusion_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        numerical: torch.Tensor,\n",
        "        author_idx: torch.Tensor,\n",
        "        category_idx: torch.Tensor,\n",
        "        market_regime_idx: torch.Tensor,\n",
        "        sector_idx: torch.Tensor,\n",
        "        market_cap_idx: torch.Tensor,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        # Get BERT [CLS] embedding\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = bert_output.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Encode features\n",
        "        num_embedding = self.numerical_encoder(numerical)\n",
        "        author_emb = self.author_embedding(author_idx)\n",
        "        category_emb = self.category_embedding(category_idx)\n",
        "        regime_emb = self.market_regime_embedding(market_regime_idx)\n",
        "        sector_emb = self.sector_embedding(sector_idx)\n",
        "        market_cap_emb = self.market_cap_embedding(market_cap_idx)\n",
        "\n",
        "        # Fusion\n",
        "        combined = torch.cat(\n",
        "            [cls_embedding, num_embedding, author_emb, category_emb, regime_emb, sector_emb, market_cap_emb],\n",
        "            dim=1,\n",
        "        )\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        output = {\"logits\": logits}\n",
        "        if labels is not None:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            output[\"loss\"] = loss\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_config(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"num_numerical_features\": self.num_numerical_features,\n",
        "            \"num_authors\": self.num_authors,\n",
        "            \"num_categories\": self.num_categories,\n",
        "            \"num_market_regimes\": self.num_market_regimes,\n",
        "            \"num_sectors\": self.num_sectors,\n",
        "            \"num_market_caps\": self.num_market_caps,\n",
        "            \"num_classes\": self.num_classes,\n",
        "            \"finbert_model\": self.finbert_model_name,\n",
        "            \"freeze_bert\": self.freeze_bert,\n",
        "            \"dropout\": self.dropout_prob,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Trainer & Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WeightedTrainer(Trainer):\n",
        "    \"\"\"Custom Trainer with class-weighted cross-entropy loss.\"\"\"\n",
        "\n",
        "    def __init__(self, class_weights: torch.Tensor, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[\"logits\"]\n",
        "\n",
        "        reduction = \"sum\" if num_items_in_batch is not None else \"mean\"\n",
        "        loss = F.cross_entropy(\n",
        "            logits,\n",
        "            labels,\n",
        "            weight=self.class_weights.to(logits.device),\n",
        "            reduction=reduction,\n",
        "        )\n",
        "\n",
        "        if num_items_in_batch is not None:\n",
        "            loss = loss / num_items_in_batch\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
        "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    data_path: str,\n",
        "    output_dir: str = \"./model_output\",\n",
        "    num_epochs: int = DEFAULT_NUM_EPOCHS,\n",
        "    batch_size: int = DEFAULT_BATCH_SIZE,\n",
        "    learning_rate: float = DEFAULT_LEARNING_RATE,\n",
        "    freeze_bert: bool = False,\n",
        "    dropout: float = DEFAULT_DROPOUT,\n",
        "):\n",
        "    \"\"\"Train the FinBERT multi-modal tweet classifier.\"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Step 1: Load and filter data\n",
        "    logger.info(f\"Loading data from {data_path}\")\n",
        "    df = load_enriched_data(data_path)\n",
        "    df_reliable = filter_reliable(df)\n",
        "    logger.info(f\"After filtering: {len(df_reliable)} reliable samples\")\n",
        "\n",
        "    # Step 2: Split by hash\n",
        "    logger.info(\"Splitting data...\")\n",
        "    df_train, df_val, df_test = split_by_hash(df_reliable)\n",
        "    logger.info(f\"Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n",
        "\n",
        "    # Step 3: Create categorical encodings\n",
        "    logger.info(\"Creating categorical encodings...\")\n",
        "    encodings = create_categorical_encodings(df_train)\n",
        "    logger.info(f\"Authors: {encodings['num_authors']}, Categories: {encodings['num_categories']}\")\n",
        "\n",
        "    # Step 4: Initialize tokenizer\n",
        "    logger.info(f\"Loading tokenizer from {FINBERT_MODEL_NAME}\")\n",
        "    tokenizer = BertTokenizer.from_pretrained(FINBERT_MODEL_NAME)\n",
        "\n",
        "    # Step 5: Create datasets\n",
        "    logger.info(\"Creating training dataset...\")\n",
        "    train_dataset, scaler = create_dataset_from_df(df_train, tokenizer, encodings, fit_scaler=True)\n",
        "    logger.info(f\"Training dataset: {len(train_dataset)} samples\")\n",
        "\n",
        "    logger.info(\"Creating validation dataset...\")\n",
        "    val_dataset, _ = create_dataset_from_df(df_val, tokenizer, encodings, scaler=scaler, fit_scaler=False)\n",
        "    logger.info(f\"Validation dataset: {len(val_dataset)} samples\")\n",
        "\n",
        "    # Step 6: Compute class weights\n",
        "    logger.info(\"Computing class weights...\")\n",
        "    class_weights = compute_class_weights(df_train[TARGET_COLUMN])\n",
        "    class_weights_tensor = weights_to_tensor(class_weights)\n",
        "    for cls, weight in class_weights.items():\n",
        "        logger.info(f\"  Class {LABEL_MAP_INV[cls]}: weight={weight:.3f}\")\n",
        "\n",
        "    # Step 7: Initialize model\n",
        "    logger.info(\"Initializing FinBERTMultiModal model...\")\n",
        "    model = FinBERTMultiModal(\n",
        "        num_numerical_features=len(NUMERICAL_FEATURES),\n",
        "        num_authors=encodings[\"num_authors\"],\n",
        "        num_categories=encodings[\"num_categories\"],\n",
        "        num_market_regimes=encodings[\"num_market_regimes\"],\n",
        "        num_sectors=encodings[\"num_sectors\"],\n",
        "        num_market_caps=encodings[\"num_market_caps\"],\n",
        "        freeze_bert=freeze_bert,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "\n",
        "    if freeze_bert:\n",
        "        logger.info(\"BERT parameters are FROZEN\")\n",
        "    else:\n",
        "        logger.info(\"BERT parameters are TRAINABLE (full fine-tuning)\")\n",
        "\n",
        "    # Step 8: Create training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=str(output_dir),\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size * 2,\n",
        "        num_train_epochs=num_epochs,\n",
        "        weight_decay=DEFAULT_WEIGHT_DECAY,\n",
        "        warmup_ratio=DEFAULT_WARMUP_RATIO,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "        fp16=torch.cuda.is_available(),  # Enable mixed precision on GPU\n",
        "        logging_steps=50,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\",\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    # Step 9: Initialize trainer\n",
        "    logger.info(\"Initializing WeightedTrainer...\")\n",
        "    trainer = WeightedTrainer(\n",
        "        class_weights=class_weights_tensor,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Step 10: Train!\n",
        "    logger.info(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Step 11: Save artifacts\n",
        "    logger.info(f\"Saving model to {output_dir}\")\n",
        "    trainer.save_model(str(output_dir / \"final\"))\n",
        "\n",
        "    # Save scaler\n",
        "    joblib.dump(scaler, output_dir / \"scaler.pkl\")\n",
        "    # Save encodings\n",
        "    joblib.dump(encodings, output_dir / \"encodings.pkl\")\n",
        "    # Save model config\n",
        "    with open(output_dir / \"model_config.json\", \"w\") as f:\n",
        "        json.dump(model.get_config(), f, indent=2)\n",
        "\n",
        "    logger.info(\"Saving preprocessing artifacts...\")\n",
        "\n",
        "    # Step 12: Final evaluation\n",
        "    logger.info(\"Running final evaluation on validation set...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "    logger.info(\"Validation results:\")\n",
        "    for key, value in eval_results.items():\n",
        "        logger.info(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "    logger.info(\"Training complete!\")\n",
        "    return model, trainer, encodings, scaler, df_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training parameters - modify as needed\n",
        "TRAINING_CONFIG = {\n",
        "    \"data_path\": DATA_FILE,\n",
        "    \"output_dir\": \"./finbert_tweet_classifier\",\n",
        "    \"num_epochs\": 5,\n",
        "    \"batch_size\": 16,  # Increase to 32 for T4/V100 GPU\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"freeze_bert\": False,  # Set to True for faster training (but lower accuracy)\n",
        "    \"dropout\": 0.3,\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "for k, v in TRAINING_CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training\n",
        "model, trainer, encodings, scaler, df_test = train(**TRAINING_CONFIG)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create test dataset\n",
        "tokenizer = BertTokenizer.from_pretrained(FINBERT_MODEL_NAME)\n",
        "test_dataset, _ = create_dataset_from_df(df_test, tokenizer, encodings, scaler=scaler, fit_scaler=False)\n",
        "\n",
        "# Get predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(labels, preds, target_names=[\"SELL\", \"HOLD\", \"BUY\"]))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"SELL\", \"HOLD\", \"BUY\"], yticklabels=[\"SELL\", \"HOLD\", \"BUY\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Download Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zip and download the model\n",
        "import shutil\n",
        "\n",
        "output_dir = \"./finbert_tweet_classifier\"\n",
        "zip_path = \"finbert_tweet_classifier.zip\"\n",
        "\n",
        "shutil.make_archive(\"finbert_tweet_classifier\", \"zip\", output_dir)\n",
        "print(f\"Model zipped to {zip_path}\")\n",
        "\n",
        "# Download\n",
        "files.download(zip_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Save to Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !cp -r ./finbert_tweet_classifier /content/drive/MyDrive/\n",
        "\n",
        "# ============================================================\n",
        "# Load trained model for inference (example function)\n",
        "# ============================================================\n",
        "\n",
        "def load_trained_model(model_dir: str):\n",
        "    \"\"\"Load a trained model for inference.\"\"\"\n",
        "    model_dir = Path(model_dir)\n",
        "    \n",
        "    # Load config\n",
        "    with open(model_dir / \"model_config.json\") as f:\n",
        "        config = json.load(f)\n",
        "    \n",
        "    # Load encodings\n",
        "    encodings = joblib.load(model_dir / \"encodings.pkl\")\n",
        "    \n",
        "    # Load scaler\n",
        "    scaler = joblib.load(model_dir / \"scaler.pkl\")\n",
        "    \n",
        "    # Initialize model with config\n",
        "    model = FinBERTMultiModal(\n",
        "        num_numerical_features=config[\"num_numerical_features\"],\n",
        "        num_authors=config[\"num_authors\"],\n",
        "        num_categories=config[\"num_categories\"],\n",
        "        num_market_regimes=config[\"num_market_regimes\"],\n",
        "        num_sectors=config[\"num_sectors\"],\n",
        "        num_market_caps=config[\"num_market_caps\"],\n",
        "        freeze_bert=config[\"freeze_bert\"],\n",
        "        dropout=config[\"dropout\"],\n",
        "    )\n",
        "    \n",
        "    # Load weights\n",
        "    final_dir = model_dir / \"final\"\n",
        "    weight_file = final_dir / \"pytorch_model.bin\"\n",
        "    if weight_file.exists():\n",
        "        state_dict = torch.load(weight_file, map_location=\"cpu\")\n",
        "        model.load_state_dict(state_dict)\n",
        "    \n",
        "    model.eval()\n",
        "    return model, encodings, scaler\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# model, encodings, scaler = load_trained_model(\"./finbert_tweet_classifier\")\n",
        "print(\"Notebook complete! Use load_trained_model() to reload a saved model.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
